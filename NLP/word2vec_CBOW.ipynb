{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTCNvi3cCLI1",
        "outputId": "41ecd278-2fdd-4095-d694-0f4429a3db42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=\"\"\"Machine Learning:Machine Learning is basically the study/process which provides the system(computer) to learn automatically on its own through experiences it had and improve accordingly without being explicitly programmed. ML is an application or subset of AI. ML focuses on the development of programs so that it can access data to use it for itself. The entire process makes observations on data to identify the possible patterns being formed and make better future decisions as per the examples provided to them. The major aim of ML is to allow the systems to learn by themselves through experience without any kind of human intervention or assistance.Deep Learning:\\nDeep Learning is basically a sub-part of the broader family of Machine Learning which makes use of Neural Networks(similar to the neurons working in our brain) to mimic human brain-like behavior. DL algorithms focus on information processing patterns mechanism to possibly identify the patterns just like our human brain does and classifies the information accordingly. DL works on larger sets of data when compared to ML and the prediction mechanism is self-administered by machines.\n",
        "Natural Language Processing (NLP) has seen significant advancements with various techniques for representing words in a way that machines can understand. One such powerful technique is GloVe (Global Vectors for Word Representation). This article delves into how GloVe improves upon traditional word embedding techniques and provides a practical guide on using pre-trained GloVe word vectors\"\"\""
      ],
      "metadata": {
        "id": "v8OvX_DRje7T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Okn04GBqKlx4",
        "outputId": "e057a342-0801-431f-aef1-f7a1eb5668c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1545"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###removing special characters"
      ],
      "metadata": {
        "id": "QOPIdHGmcoLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in \"!@#$%^&*()_-+=:;<>?|\\{}[]\\n'\":\n",
        "    data=data.replace(i,'  ')\n",
        "data=data.replace('  \" ','')"
      ],
      "metadata": {
        "id": "c85MprN9Ko3e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VpZWmJ2edhpY",
        "outputId": "9a2a7472-8550-4001-8ee3-a683f3aa5303"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine Learning  Machine Learning is basically the study/process which provides the system  compute'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###word index"
      ],
      "metadata": {
        "id": "EQaLNsPVe5r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=word_tokenize(data[:100])"
      ],
      "metadata": {
        "id": "5X4D0p5je5AK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiKfY-zriuzS",
        "outputId": "30a9e231-9157-417b-92ec-1e972e5a975d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Machine',\n",
              " 'Learning',\n",
              " 'Machine',\n",
              " 'Learning',\n",
              " 'is',\n",
              " 'basically',\n",
              " 'the',\n",
              " 'study/process',\n",
              " 'which',\n",
              " 'provides',\n",
              " 'the',\n",
              " 'system',\n",
              " 'compute']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word for word in words if len(word) != 0]"
      ],
      "metadata": {
        "id": "5uqVJCCA6qql"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index={word:i for i, word in enumerate(set(words))}\n",
        "\n",
        "index_word={word_index[word]:word for word in word_index}"
      ],
      "metadata": {
        "id": "Faxf6JhAhisQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index['system']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k6Gi-Znh6YJ",
        "outputId": "d3d8522f-b622-4de5-a360-95d3d4b2eed4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_word={word_index[word]:word for word in word_index}"
      ],
      "metadata": {
        "id": "DWBw1h00iU39"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent=sent_tokenize(data)\n",
        "w=word_tokenize(data)"
      ],
      "metadata": {
        "id": "dhZPd4XXk0Sp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent)\n",
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2ResSlzk50k",
        "outputId": "cb4f87ea-7102-4785-c0fa-9e337d25871a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine Learning  Machine Learning is basically the study/process which provides the system  computer   to learn automatically on its own through experiences it had and improve accordingly without being explicitly programmed.', 'ML is an application or subset of AI.', 'ML focuses on the development of programs so that it can access data to use it for itself.', 'The entire process makes observations on data to identify the possible patterns being formed and make better future decisions as per the examples provided to them.', 'The major aim of ML is to allow the systems to learn by themselves through experience without any kind of human intervention or assistance.Deep Learning    Deep Learning is basically a sub  part of the broader family of Machine Learning which makes use of\\xa0Neural Networks  similar to the neurons working in our brain   to mimic human brain  like behavior.', 'DL algorithms focus on\\xa0information processing patterns\\xa0mechanism to possibly identify the patterns just like our human brain does and classifies the information accordingly.', 'DL works on larger sets of data when compared to ML and the\\xa0prediction mechanism is self  administered by machines.', 'Natural Language Processing   NLP   has seen significant advancements with various techniques for representing words in a way that machines can understand.', 'One such powerful technique is GloVe   Global Vectors for Word Representation  .', 'This article delves into how GloVe improves upon traditional word embedding techniques and provides a practical guide on using pre  trained GloVe word vectors']\n",
            "['Machine', 'Learning', 'Machine', 'Learning', 'is', 'basically', 'the', 'study/process', 'which', 'provides', 'the', 'system', 'computer', 'to', 'learn', 'automatically', 'on', 'its', 'own', 'through', 'experiences', 'it', 'had', 'and', 'improve', 'accordingly', 'without', 'being', 'explicitly', 'programmed', '.', 'ML', 'is', 'an', 'application', 'or', 'subset', 'of', 'AI', '.', 'ML', 'focuses', 'on', 'the', 'development', 'of', 'programs', 'so', 'that', 'it', 'can', 'access', 'data', 'to', 'use', 'it', 'for', 'itself', '.', 'The', 'entire', 'process', 'makes', 'observations', 'on', 'data', 'to', 'identify', 'the', 'possible', 'patterns', 'being', 'formed', 'and', 'make', 'better', 'future', 'decisions', 'as', 'per', 'the', 'examples', 'provided', 'to', 'them', '.', 'The', 'major', 'aim', 'of', 'ML', 'is', 'to', 'allow', 'the', 'systems', 'to', 'learn', 'by', 'themselves', 'through', 'experience', 'without', 'any', 'kind', 'of', 'human', 'intervention', 'or', 'assistance.Deep', 'Learning', 'Deep', 'Learning', 'is', 'basically', 'a', 'sub', 'part', 'of', 'the', 'broader', 'family', 'of', 'Machine', 'Learning', 'which', 'makes', 'use', 'of', 'Neural', 'Networks', 'similar', 'to', 'the', 'neurons', 'working', 'in', 'our', 'brain', 'to', 'mimic', 'human', 'brain', 'like', 'behavior', '.', 'DL', 'algorithms', 'focus', 'on', 'information', 'processing', 'patterns', 'mechanism', 'to', 'possibly', 'identify', 'the', 'patterns', 'just', 'like', 'our', 'human', 'brain', 'does', 'and', 'classifies', 'the', 'information', 'accordingly', '.', 'DL', 'works', 'on', 'larger', 'sets', 'of', 'data', 'when', 'compared', 'to', 'ML', 'and', 'the', 'prediction', 'mechanism', 'is', 'self', 'administered', 'by', 'machines', '.', 'Natural', 'Language', 'Processing', 'NLP', 'has', 'seen', 'significant', 'advancements', 'with', 'various', 'techniques', 'for', 'representing', 'words', 'in', 'a', 'way', 'that', 'machines', 'can', 'understand', '.', 'One', 'such', 'powerful', 'technique', 'is', 'GloVe', 'Global', 'Vectors', 'for', 'Word', 'Representation', '.', 'This', 'article', 'delves', 'into', 'how', 'GloVe', 'improves', 'upon', 'traditional', 'word', 'embedding', 'techniques', 'and', 'provides', 'a', 'practical', 'guide', 'on', 'using', 'pre', 'trained', 'GloVe', 'word', 'vectors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=word_tokenize(sent[0])\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW6QIAULlPAn",
        "outputId": "3addcf1e-9b39-4796-bad4-22ffae9c54c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine', 'Learning', 'Machine', 'Learning', 'is', 'basically', 'the', 'study/process', 'which', 'provides', 'the', 'system', 'computer', 'to', 'learn', 'automatically', 'on', 'its', 'own', 'through', 'experiences', 'it', 'had', 'and', 'improve', 'accordingly', 'without', 'being', 'explicitly', 'programmed', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s=[word_tokenize(sent) for sent in sent_tokenize(data)]\n",
        "print(s[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58gRhzYNpXNv",
        "outputId": "0e006906-9e1b-48c8-81d8-63f6f744ce70"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine', 'Learning', 'Machine', 'Learning', 'is', 'basically', 'the', 'study/process', 'which', 'provides', 'the', 'system', 'computer', 'to', 'learn', 'automatically', 'on', 'its', 'own', 'through', 'experiences', 'it', 'had', 'and', 'improve', 'accordingly', 'without', 'being', 'explicitly', 'programmed', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###create trainning data"
      ],
      "metadata": {
        "id": "X6HWYBKrmwdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "npjgGt-OlYao",
        "outputId": "15c7c4f2-11f4-4689-902b-6d19831b8ea9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine Learning Machine Learning is basically the study/process which provides the system computer to learn automatically on its own through experiences it had and improve accordingly without being explicitly programmed .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size=2\n",
        "for sent in s:\n",
        "  for i in range (len(sent)-window_size*2):\n",
        "      print(sent[i:i+window_size])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dln9iifuslEe",
        "outputId": "5eed3cb7-88f8-4672-ce1f-5e36e761089b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine', 'Learning']\n",
            "['Learning', 'Machine']\n",
            "['Machine', 'Learning']\n",
            "['Learning', 'is']\n",
            "['is', 'basically']\n",
            "['basically', 'the']\n",
            "['the', 'study/process']\n",
            "['study/process', 'which']\n",
            "['which', 'provides']\n",
            "['provides', 'the']\n",
            "['the', 'system']\n",
            "['system', 'computer']\n",
            "['computer', 'to']\n",
            "['to', 'learn']\n",
            "['learn', 'automatically']\n",
            "['automatically', 'on']\n",
            "['on', 'its']\n",
            "['its', 'own']\n",
            "['own', 'through']\n",
            "['through', 'experiences']\n",
            "['experiences', 'it']\n",
            "['it', 'had']\n",
            "['had', 'and']\n",
            "['and', 'improve']\n",
            "['improve', 'accordingly']\n",
            "['accordingly', 'without']\n",
            "['without', 'being']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features=[]\n",
        "labels=[]\n",
        "\n",
        "\n",
        "window_size=2\n",
        "\n",
        "for sent in s:\n",
        "  for i in range(len(sent)-(window_size)*2):\n",
        "      features.append(sent[i:i+window_size]+ sent[i+window_size+1: window_size*2+1])\n",
        "      labels.append(sent[i+window_size])\n"
      ],
      "metadata": {
        "id": "bdU9IpRcnKrG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[0],labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5NNYA7SnQbC",
        "outputId": "4351609e-d146-4b94-c958-1cff9027b463"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Machine', 'Learning', 'Learning', 'is'], 'Machine')"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(features[i],labels[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqLfjmE1ucrY",
        "outputId": "dfb83268-65d4-44a5-82ad-4ce620174865"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine', 'Learning', 'Learning', 'is'] Machine\n",
            "['Learning', 'Machine', 'is'] Learning\n",
            "['Machine', 'Learning'] is\n",
            "['Learning', 'is'] basically\n",
            "['is', 'basically'] the\n",
            "['basically', 'the'] study/process\n",
            "['the', 'study/process'] which\n",
            "['study/process', 'which'] provides\n",
            "['which', 'provides'] the\n",
            "['provides', 'the'] system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=[]\n",
        "\n",
        "for feature in features:\n",
        "  enc=np.zeros(len(word_index))\n",
        "  for word in feature:\n",
        "    if word in word_index:\n",
        "       enc[word_index[word]]=1\n",
        "    else:\n",
        "      print(word,\"not found\")\n",
        "  X_train.append(enc)\n",
        "X_train=np.array(X_train)\n"
      ],
      "metadata": {
        "id": "NSWleOhmukZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d96722-0ee8-4e4d-cbdb-d13963f5afac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer not found\n",
            "computer not found\n",
            "to not found\n",
            "to not found\n",
            "learn not found\n",
            "learn not found\n",
            "automatically not found\n",
            "automatically not found\n",
            "on not found\n",
            "on not found\n",
            "its not found\n",
            "its not found\n",
            "own not found\n",
            "own not found\n",
            "through not found\n",
            "through not found\n",
            "experiences not found\n",
            "experiences not found\n",
            "it not found\n",
            "it not found\n",
            "had not found\n",
            "had not found\n",
            "and not found\n",
            "and not found\n",
            "improve not found\n",
            "improve not found\n",
            "accordingly not found\n",
            "accordingly not found\n",
            "without not found\n",
            "without not found\n",
            "being not found\n",
            "ML not found\n",
            "application not found\n",
            "or not found\n",
            "an not found\n",
            "or not found\n",
            "an not found\n",
            "application not found\n",
            "application not found\n",
            "or not found\n",
            "or not found\n",
            "subset not found\n",
            "ML not found\n",
            "focuses not found\n",
            "development not found\n",
            "focuses not found\n",
            "on not found\n",
            "development not found\n",
            "on not found\n",
            "development not found\n",
            "development not found\n",
            "of not found\n",
            "of not found\n",
            "programs not found\n",
            "programs not found\n",
            "so not found\n",
            "so not found\n",
            "that not found\n",
            "that not found\n",
            "it not found\n",
            "it not found\n",
            "can not found\n",
            "can not found\n",
            "access not found\n",
            "access not found\n",
            "data not found\n",
            "data not found\n",
            "to not found\n",
            "to not found\n",
            "use not found\n",
            "use not found\n",
            "it not found\n",
            "The not found\n",
            "entire not found\n",
            "makes not found\n",
            "observations not found\n",
            "entire not found\n",
            "process not found\n",
            "observations not found\n",
            "process not found\n",
            "makes not found\n",
            "makes not found\n",
            "observations not found\n",
            "observations not found\n",
            "on not found\n",
            "on not found\n",
            "data not found\n",
            "data not found\n",
            "to not found\n",
            "to not found\n",
            "identify not found\n",
            "identify not found\n",
            "possible not found\n",
            "possible not found\n",
            "patterns not found\n",
            "patterns not found\n",
            "being not found\n",
            "being not found\n",
            "formed not found\n",
            "formed not found\n",
            "and not found\n",
            "and not found\n",
            "make not found\n",
            "make not found\n",
            "better not found\n",
            "better not found\n",
            "future not found\n",
            "future not found\n",
            "decisions not found\n",
            "decisions not found\n",
            "as not found\n",
            "as not found\n",
            "per not found\n",
            "per not found\n",
            "examples not found\n",
            "examples not found\n",
            "provided not found\n",
            "The not found\n",
            "major not found\n",
            "of not found\n",
            "ML not found\n",
            "major not found\n",
            "aim not found\n",
            "ML not found\n",
            "aim not found\n",
            "of not found\n",
            "of not found\n",
            "ML not found\n",
            "ML not found\n",
            "to not found\n",
            "to not found\n",
            "allow not found\n",
            "allow not found\n",
            "systems not found\n",
            "systems not found\n",
            "to not found\n",
            "to not found\n",
            "learn not found\n",
            "learn not found\n",
            "by not found\n",
            "by not found\n",
            "themselves not found\n",
            "themselves not found\n",
            "through not found\n",
            "through not found\n",
            "experience not found\n",
            "experience not found\n",
            "without not found\n",
            "without not found\n",
            "any not found\n",
            "any not found\n",
            "kind not found\n",
            "kind not found\n",
            "of not found\n",
            "of not found\n",
            "human not found\n",
            "human not found\n",
            "intervention not found\n",
            "intervention not found\n",
            "or not found\n",
            "or not found\n",
            "assistance.Deep not found\n",
            "assistance.Deep not found\n",
            "Deep not found\n",
            "Deep not found\n",
            "a not found\n",
            "a not found\n",
            "sub not found\n",
            "sub not found\n",
            "part not found\n",
            "part not found\n",
            "of not found\n",
            "of not found\n",
            "broader not found\n",
            "broader not found\n",
            "family not found\n",
            "family not found\n",
            "of not found\n",
            "of not found\n",
            "makes not found\n",
            "makes not found\n",
            "use not found\n",
            "use not found\n",
            "of not found\n",
            "of not found\n",
            "Neural not found\n",
            "Neural not found\n",
            "Networks not found\n",
            "Networks not found\n",
            "similar not found\n",
            "similar not found\n",
            "to not found\n",
            "to not found\n",
            "neurons not found\n",
            "neurons not found\n",
            "working not found\n",
            "working not found\n",
            "in not found\n",
            "in not found\n",
            "our not found\n",
            "our not found\n",
            "brain not found\n",
            "brain not found\n",
            "to not found\n",
            "to not found\n",
            "mimic not found\n",
            "mimic not found\n",
            "human not found\n",
            "human not found\n",
            "brain not found\n",
            "DL not found\n",
            "algorithms not found\n",
            "on not found\n",
            "information not found\n",
            "algorithms not found\n",
            "focus not found\n",
            "information not found\n",
            "focus not found\n",
            "on not found\n",
            "on not found\n",
            "information not found\n",
            "information not found\n",
            "processing not found\n",
            "processing not found\n",
            "patterns not found\n",
            "patterns not found\n",
            "mechanism not found\n",
            "mechanism not found\n",
            "to not found\n",
            "to not found\n",
            "possibly not found\n",
            "possibly not found\n",
            "identify not found\n",
            "identify not found\n",
            "patterns not found\n",
            "patterns not found\n",
            "just not found\n",
            "just not found\n",
            "like not found\n",
            "like not found\n",
            "our not found\n",
            "our not found\n",
            "human not found\n",
            "human not found\n",
            "brain not found\n",
            "brain not found\n",
            "does not found\n",
            "does not found\n",
            "and not found\n",
            "and not found\n",
            "classifies not found\n",
            "classifies not found\n",
            "DL not found\n",
            "works not found\n",
            "larger not found\n",
            "sets not found\n",
            "works not found\n",
            "on not found\n",
            "sets not found\n",
            "on not found\n",
            "larger not found\n",
            "larger not found\n",
            "sets not found\n",
            "sets not found\n",
            "of not found\n",
            "of not found\n",
            "data not found\n",
            "data not found\n",
            "when not found\n",
            "when not found\n",
            "compared not found\n",
            "compared not found\n",
            "to not found\n",
            "to not found\n",
            "ML not found\n",
            "ML not found\n",
            "and not found\n",
            "and not found\n",
            "prediction not found\n",
            "prediction not found\n",
            "mechanism not found\n",
            "mechanism not found\n",
            "self not found\n",
            "self not found\n",
            "administered not found\n",
            "Natural not found\n",
            "Language not found\n",
            "NLP not found\n",
            "has not found\n",
            "Language not found\n",
            "Processing not found\n",
            "has not found\n",
            "Processing not found\n",
            "NLP not found\n",
            "NLP not found\n",
            "has not found\n",
            "has not found\n",
            "seen not found\n",
            "seen not found\n",
            "significant not found\n",
            "significant not found\n",
            "advancements not found\n",
            "advancements not found\n",
            "with not found\n",
            "with not found\n",
            "various not found\n",
            "various not found\n",
            "techniques not found\n",
            "techniques not found\n",
            "for not found\n",
            "for not found\n",
            "representing not found\n",
            "representing not found\n",
            "words not found\n",
            "words not found\n",
            "in not found\n",
            "in not found\n",
            "a not found\n",
            "a not found\n",
            "way not found\n",
            "way not found\n",
            "that not found\n",
            "that not found\n",
            "machines not found\n",
            "One not found\n",
            "such not found\n",
            "technique not found\n",
            "such not found\n",
            "powerful not found\n",
            "powerful not found\n",
            "technique not found\n",
            "technique not found\n",
            "GloVe not found\n",
            "GloVe not found\n",
            "Global not found\n",
            "Global not found\n",
            "Vectors not found\n",
            "Vectors not found\n",
            "for not found\n",
            "This not found\n",
            "article not found\n",
            "into not found\n",
            "how not found\n",
            "article not found\n",
            "delves not found\n",
            "how not found\n",
            "delves not found\n",
            "into not found\n",
            "into not found\n",
            "how not found\n",
            "how not found\n",
            "GloVe not found\n",
            "GloVe not found\n",
            "improves not found\n",
            "improves not found\n",
            "upon not found\n",
            "upon not found\n",
            "traditional not found\n",
            "traditional not found\n",
            "word not found\n",
            "word not found\n",
            "embedding not found\n",
            "embedding not found\n",
            "techniques not found\n",
            "techniques not found\n",
            "and not found\n",
            "and not found\n",
            "a not found\n",
            "a not found\n",
            "practical not found\n",
            "practical not found\n",
            "guide not found\n",
            "guide not found\n",
            "on not found\n",
            "on not found\n",
            "using not found\n",
            "using not found\n",
            "pre not found\n",
            "pre not found\n",
            "trained not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "for feature in features:\n",
        "    enc = np.zeros(len(word_index))\n",
        "    for word in feature:\n",
        "        enc[word_index[word]] = 1\n",
        "\n",
        "    X_train.append(enc)\n",
        "\n",
        "X_train = np.array(X_train)"
      ],
      "metadata": {
        "id": "1o7DBS5FpQvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = []\n",
        "\n",
        "for label in labels:\n",
        "    if label in word_index:\n",
        "        enc = np.zeros(len(word_index))\n",
        "        enc[word_index[label]] = 1\n",
        "        y_train.append(enc)\n",
        "    else:\n",
        "        print(label,'not found in word_index.')\n",
        "y_train = np.array(y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIY8XxjJ5Izm",
        "outputId": "9a7b325a-b82d-4930-8612-d9eee14e503d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer not found in word_index.\n",
            "to not found in word_index.\n",
            "learn not found in word_index.\n",
            "automatically not found in word_index.\n",
            "on not found in word_index.\n",
            "its not found in word_index.\n",
            "own not found in word_index.\n",
            "through not found in word_index.\n",
            "experiences not found in word_index.\n",
            "it not found in word_index.\n",
            "had not found in word_index.\n",
            "and not found in word_index.\n",
            "improve not found in word_index.\n",
            "accordingly not found in word_index.\n",
            "without not found in word_index.\n",
            "being not found in word_index.\n",
            "explicitly not found in word_index.\n",
            "an not found in word_index.\n",
            "application not found in word_index.\n",
            "or not found in word_index.\n",
            "subset not found in word_index.\n",
            "of not found in word_index.\n",
            "on not found in word_index.\n",
            "development not found in word_index.\n",
            "of not found in word_index.\n",
            "programs not found in word_index.\n",
            "so not found in word_index.\n",
            "that not found in word_index.\n",
            "it not found in word_index.\n",
            "can not found in word_index.\n",
            "access not found in word_index.\n",
            "data not found in word_index.\n",
            "to not found in word_index.\n",
            "use not found in word_index.\n",
            "it not found in word_index.\n",
            "for not found in word_index.\n",
            "process not found in word_index.\n",
            "makes not found in word_index.\n",
            "observations not found in word_index.\n",
            "on not found in word_index.\n",
            "data not found in word_index.\n",
            "to not found in word_index.\n",
            "identify not found in word_index.\n",
            "possible not found in word_index.\n",
            "patterns not found in word_index.\n",
            "being not found in word_index.\n",
            "formed not found in word_index.\n",
            "and not found in word_index.\n",
            "make not found in word_index.\n",
            "better not found in word_index.\n",
            "future not found in word_index.\n",
            "decisions not found in word_index.\n",
            "as not found in word_index.\n",
            "per not found in word_index.\n",
            "examples not found in word_index.\n",
            "provided not found in word_index.\n",
            "to not found in word_index.\n",
            "aim not found in word_index.\n",
            "of not found in word_index.\n",
            "ML not found in word_index.\n",
            "to not found in word_index.\n",
            "allow not found in word_index.\n",
            "systems not found in word_index.\n",
            "to not found in word_index.\n",
            "learn not found in word_index.\n",
            "by not found in word_index.\n",
            "themselves not found in word_index.\n",
            "through not found in word_index.\n",
            "experience not found in word_index.\n",
            "without not found in word_index.\n",
            "any not found in word_index.\n",
            "kind not found in word_index.\n",
            "of not found in word_index.\n",
            "human not found in word_index.\n",
            "intervention not found in word_index.\n",
            "or not found in word_index.\n",
            "assistance.Deep not found in word_index.\n",
            "Deep not found in word_index.\n",
            "a not found in word_index.\n",
            "sub not found in word_index.\n",
            "part not found in word_index.\n",
            "of not found in word_index.\n",
            "broader not found in word_index.\n",
            "family not found in word_index.\n",
            "of not found in word_index.\n",
            "makes not found in word_index.\n",
            "use not found in word_index.\n",
            "of not found in word_index.\n",
            "Neural not found in word_index.\n",
            "Networks not found in word_index.\n",
            "similar not found in word_index.\n",
            "to not found in word_index.\n",
            "neurons not found in word_index.\n",
            "working not found in word_index.\n",
            "in not found in word_index.\n",
            "our not found in word_index.\n",
            "brain not found in word_index.\n",
            "to not found in word_index.\n",
            "mimic not found in word_index.\n",
            "human not found in word_index.\n",
            "brain not found in word_index.\n",
            "like not found in word_index.\n",
            "focus not found in word_index.\n",
            "on not found in word_index.\n",
            "information not found in word_index.\n",
            "processing not found in word_index.\n",
            "patterns not found in word_index.\n",
            "mechanism not found in word_index.\n",
            "to not found in word_index.\n",
            "possibly not found in word_index.\n",
            "identify not found in word_index.\n",
            "patterns not found in word_index.\n",
            "just not found in word_index.\n",
            "like not found in word_index.\n",
            "our not found in word_index.\n",
            "human not found in word_index.\n",
            "brain not found in word_index.\n",
            "does not found in word_index.\n",
            "and not found in word_index.\n",
            "classifies not found in word_index.\n",
            "information not found in word_index.\n",
            "on not found in word_index.\n",
            "larger not found in word_index.\n",
            "sets not found in word_index.\n",
            "of not found in word_index.\n",
            "data not found in word_index.\n",
            "when not found in word_index.\n",
            "compared not found in word_index.\n",
            "to not found in word_index.\n",
            "ML not found in word_index.\n",
            "and not found in word_index.\n",
            "prediction not found in word_index.\n",
            "mechanism not found in word_index.\n",
            "self not found in word_index.\n",
            "administered not found in word_index.\n",
            "by not found in word_index.\n",
            "Processing not found in word_index.\n",
            "NLP not found in word_index.\n",
            "has not found in word_index.\n",
            "seen not found in word_index.\n",
            "significant not found in word_index.\n",
            "advancements not found in word_index.\n",
            "with not found in word_index.\n",
            "various not found in word_index.\n",
            "techniques not found in word_index.\n",
            "for not found in word_index.\n",
            "representing not found in word_index.\n",
            "words not found in word_index.\n",
            "in not found in word_index.\n",
            "a not found in word_index.\n",
            "way not found in word_index.\n",
            "that not found in word_index.\n",
            "machines not found in word_index.\n",
            "can not found in word_index.\n",
            "powerful not found in word_index.\n",
            "technique not found in word_index.\n",
            "GloVe not found in word_index.\n",
            "Global not found in word_index.\n",
            "Vectors not found in word_index.\n",
            "for not found in word_index.\n",
            "Word not found in word_index.\n",
            "delves not found in word_index.\n",
            "into not found in word_index.\n",
            "how not found in word_index.\n",
            "GloVe not found in word_index.\n",
            "improves not found in word_index.\n",
            "upon not found in word_index.\n",
            "traditional not found in word_index.\n",
            "word not found in word_index.\n",
            "embedding not found in word_index.\n",
            "techniques not found in word_index.\n",
            "and not found in word_index.\n",
            "a not found in word_index.\n",
            "practical not found in word_index.\n",
            "guide not found in word_index.\n",
            "on not found in word_index.\n",
            "using not found in word_index.\n",
            "pre not found in word_index.\n",
            "trained not found in word_index.\n",
            "GloVe not found in word_index.\n"
          ]
        }
      ]
    }
  ]
}